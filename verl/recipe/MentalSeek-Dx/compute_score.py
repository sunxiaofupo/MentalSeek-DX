"""
Process Reward-Based Curriculum Reinforcement Learning (PRCRL) Reward Function
Based on the paper: Process Reward-Based Curriculum Reinforcement Learning

Reward composition:
R_diagnosis = α1 * R_cat + α2 * R_hypo + α3 * R_diff

Where:
- R_cat: Category identification reward (binary: 1 if correct, 0 otherwise)
- R_hypo: Hypothesis generation reward (rank-based: I[i <= 4] * (1 - (i-1)/4))
- R_diff: Differential diagnosis reward (binary: 1 if correct, 0 otherwise)

Supports curriculum learning with stage-aware reward scheduling across 5 epochs.
"""

import json
import logging
import os
import re
from logging.handlers import RotatingFileHandler
from pathlib import Path
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)

# Global variables for storing current training state (can be updated via environment variables or files)
_CURRENT_EPOCH_FILE = os.getenv("VERL_CURRENT_EPOCH_FILE", "/tmp/verl_current_epoch.txt")
_CURRENT_GLOBAL_STEP_FILE = os.getenv("VERL_CURRENT_GLOBAL_STEP_FILE", "/tmp/verl_current_global_step.txt")

LOG_DIR = os.path.join(os.path.dirname(__file__), "log")
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, "reward.log")

if not logger.handlers:
    handler = RotatingFileHandler(LOG_FILE, maxBytes=10 * 1024 * 1024, backupCount=3, encoding="utf-8")
    handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(message)s"))
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)
    logger.propagate = False


def _build_solution_summary(solution_str: str, max_length: int = 1024) -> str:
    normalized = solution_str.replace("\n", " ").strip()
    if len(normalized) <= max_length:
        return normalized
    return f"{normalized[:max_length]}...(len={len(normalized)})"


def _has_valid_format(solution_str: str) -> bool:
    think_match = re.search(r"<think>\s*\{.*?\}\s*</think>", solution_str, re.DOTALL)
    answer_match = re.search(r"<answer>\s*\{.*?\}\s*</answer>", solution_str, re.DOTALL)
    return bool(think_match or answer_match)


def parse_model_output(solution_str: str) -> Optional[Dict[str, Any]]:
    """
    Extract diagnosis JSON from model output
    Expected format: <answer>{...}</answer>
    """
    try:
        # Try to extract content from <answer> tag
        answer_match = re.search(r'<answer>(.*?)</answer>', solution_str, re.DOTALL)
        if not answer_match:
            # If no <answer> tag, try to parse JSON directly
            json_match = re.search(r'\{.*\}', solution_str, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            return None
        
        answer_content = answer_match.group(1).strip()
        return json.loads(answer_content)
    except (json.JSONDecodeError, AttributeError):
        return None


def calculate_reward(
    solution_str: str,
    ground_truth: Dict[str, Any],
    epoch: int = 0
) -> float:
    """
    Calculate process-based reward score according to PRCRL
    
    Args:
        solution_str: Complete output text generated by the model
        ground_truth: Dictionary containing correct answers
            - diagnostic_category_correct: Correct diagnostic category
            - disorder_code_correct: Correct disorder code (ground truth disorder)
        epoch: Current training epoch (0-4), used for curriculum learning
    Returns:
        Reward score R_diagnosis = α1 * R_cat + α2 * R_hypo + α3 * R_diff
    """
    # Parse model output
    predicted = parse_model_output(solution_str)
    if not predicted:
        logger.warning("Cannot parse model output, returning 0.0")
        return 0.0
    
    # 1. Category identification reward (R_cat)
    # R_cat = I[C_pred = C_gt]
    predicted_category = predicted.get("diagnostic_category", "")
    correct_category = ground_truth.get("diagnostic_category_correct", "")
    
    if predicted_category and correct_category:
        category_correct = predicted_category.lower() == correct_category.lower()
        R_cat = 1.0 if category_correct else 0.0
    else:
        category_correct = False
        R_cat = 0.0
    
    # 2. Hypothesis generation reward (R_hypo)
    # R_hypo = I[i <= 4] * (1 - (i-1)/4)
    # where i is the rank position of ground truth disorder in candidate list
    R_hypo = 0.0
    predicted_candidates = predicted.get("candidate_disorders", [])
    correct_code = ground_truth.get("disorder_code_correct", "")
    
    if isinstance(predicted_candidates, list) and correct_code and predicted_candidates:
        try:
            # Find the rank position of correct_code in the candidate list (1-indexed)
            rank_position = None
            for idx, candidate in enumerate(predicted_candidates):
                if str(candidate).strip() == str(correct_code).strip():
                    rank_position = idx + 1  # 1-indexed position
                    break
            
            if rank_position is not None and rank_position <= 4:
                # R_hypo = I[i <= 4] * (1 - (i-1)/4)
                R_hypo = 1.0 - (rank_position - 1) / 4.0
        except (TypeError, AttributeError) as e:
            logger.warning(f"Error calculating hypothesis reward: {e}")
            R_hypo = 0.0
    
    # 3. Differential diagnosis reward (R_diff)
    # R_diff = I[d_pred = d_gt]
    predicted_code = predicted.get("disorder_code", "")
    if predicted_code and correct_code:
        R_diff = 1.0 if str(predicted_code).strip() == str(correct_code).strip() else 0.0
    else:
        R_diff = 0.0
    
    # Get curriculum learning weights based on epoch
    # Training is divided into 5 epochs with progressive weight adjustment
    if epoch < 0:
        epoch = 0
    elif epoch >= 5:
        epoch = 4
    
    # Curriculum learning: adjust weights (α1, α2, α3) across epochs
    # Early epochs: higher α1 (focus on category)
    # Later epochs: gradually increase α2 or α3 (focus on hypothesis or differential diagnosis)
    if epoch == 0 or epoch == 1:
        # Epoch 0-1: Focus on category identification
        alpha1, alpha2, alpha3 = 1.0, 0.0, 0.0
    elif epoch == 2:
        # Epoch 2: Category + Hypothesis
        alpha1, alpha2, alpha3 = 0.5, 0.5, 0.0
    elif epoch == 3:
        # Epoch 3: Balanced
        alpha1, alpha2, alpha3 = 0.3, 0.4, 0.3
    else:  # epoch == 4
        # Epoch 4: Focus on differential diagnosis
        alpha1, alpha2, alpha3 = 0.2, 0.3, 0.5
    
    # Calculate final reward: R_diagnosis = α1 * R_cat + α2 * R_hypo + α3 * R_diff
    final_reward = alpha1 * R_cat + alpha2 * R_hypo + alpha3 * R_diff
    
    logger.info(
        "reward_score epoch=%d R_cat=%.3f R_hypo=%.3f R_diff=%.3f alpha=(%.2f,%.2f,%.2f) final=%.3f",
        epoch,
        R_cat,
        R_hypo,
        R_diff,
        alpha1,
        alpha2,
        alpha3,
        final_reward,
    )
    return final_reward




def _get_current_epoch_for_curriculum(total_epochs: int = 5) -> int:
    """
    Get current training epoch for curriculum learning
    
    Args:
        total_epochs: Total number of epochs (default 5)
        
    Returns:
        Current epoch number (0-4), returns 0 if unable to get
    """
    # Method 1: Get from environment variable
    epoch_from_env = os.getenv("VERL_CURRENT_EPOCH")
    if epoch_from_env:
        try:
            epoch = int(epoch_from_env)
            if 0 <= epoch < total_epochs:
                return epoch
        except ValueError:
            pass
    
    # Method 2: Get epoch from file
    epoch_file = Path(_CURRENT_EPOCH_FILE)
    if epoch_file.exists():
        try:
            epoch_str = epoch_file.read_text(encoding="utf-8").strip()
            epoch = int(epoch_str)
            if 0 <= epoch < total_epochs:
                return epoch
        except (ValueError, IOError) as e:
            logger.warning(f"Unable to read epoch from file: {e}")
    
    # Method 3: Calculate epoch from global_step file
    step_file = Path(_CURRENT_GLOBAL_STEP_FILE)
    if step_file.exists():
        try:
            global_step = int(step_file.read_text(encoding="utf-8").strip())
            steps_per_epoch = int(os.getenv("VERL_STEPS_PER_EPOCH", "0"))
            if steps_per_epoch > 0:
                epoch = global_step // steps_per_epoch
                if 0 <= epoch < total_epochs:
                    return epoch
        except (ValueError, IOError) as e:
            logger.warning(f"Unable to calculate epoch from global_step: {e}")
    
    return 0  # Default to epoch 0


def reward_function(
    solution_str: str,
    ground_truth: Dict[str, Any],
    **kwargs
) -> float:
    """
    Main interface compatible with reward system, returns process-based reward score.
    
    Implements Process Reward-Based Curriculum Reinforcement Learning (PRCRL):
    R_diagnosis = α1 * R_cat + α2 * R_hypo + α3 * R_diff
    
    Supports getting training epoch through the following methods:
    1. kwargs.get("current_epoch") - Get current epoch from kwargs
    2. Environment variable VERL_CURRENT_EPOCH - Get epoch from environment variable
    3. File /tmp/verl_current_epoch.txt - Read epoch from file
    4. Default value 0 (first epoch)
    """
    # Get current epoch for curriculum learning
    current_epoch = kwargs.get("current_epoch")
    if current_epoch is None:
        total_epochs = kwargs.get("total_epochs", 5)  # Default 5 epochs
        current_epoch = _get_current_epoch_for_curriculum(total_epochs)
    
    # Ensure epoch is in valid range [0, 4]
    if current_epoch < 0:
        current_epoch = 0
        logger.warning("Epoch < 0, defaulting to epoch 0")
    elif current_epoch >= 5:
        current_epoch = 4
        logger.warning(f"Epoch >= 5, defaulting to epoch 4")
    
    # Log input information
    prompt = (
        ground_truth.get("prompt")
        or ground_truth.get("input")
        or ground_truth.get("context")
        or ""
    )
    prompt_summary = _build_solution_summary(prompt)
    prompt_key = kwargs.get("prompt_key", "prompt")
    logger.info(
        "reward_prompt key=%s len=%d summary=%s",
        prompt_key,
        len(prompt),
        prompt_summary,
    )
    summary = _build_solution_summary(solution_str)
    logger.info("reward_input len=%d summary=%s", len(solution_str), summary)

    # Validate output format
    if not _has_valid_format(solution_str):
        logger.info("reward_format_invalid summary=%s", summary)
        return 0.0

    # Calculate process-based reward
    reward = calculate_reward(solution_str, ground_truth, current_epoch)
    return reward